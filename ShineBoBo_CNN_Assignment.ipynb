{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ef9383",
   "metadata": {},
   "source": [
    "# What is Convolutional Neural Network (CNN)?\n",
    "    A CNN is one of the types of deep learning model that is designed to process data like images. It looks into the small parts of the images to find unique patterns features and shapes CNNs are good at recognizing patterns, shapes and features in an image. So, they are perfect for specific tasks like image classification, object detection and facial recognization.\n",
    "\n",
    "\n",
    "# What is the role of the convolutional layer?\n",
    "    The convolutional layer is a feature detector that scans the entire image using small filters. The filters detect specific pattern in the image like edges, corners or textures by performing mathematical operations. Each filter create a map that highlights where certain pattern appears in the image.\n",
    "\n",
    "\n",
    "# Why do we use pooling layers in CNNs?\n",
    "    Pooling layers help reduce the size of feature maps while keeping the most important information. They work by taking maximum or average values from the regions of the feature map. Pooling also make network more robust by ensuring it can recoginze features no matter where they are in the image. \n",
    "\n",
    "\n",
    "# What are the purposes of flattening and fully connected layers?\n",
    "    Flattening mainly changes into a 1D vector from 2D feature map. Layers which are connected use this vector to make the final decision about what the image contains. These layers combine all the detected features to classify the image into specific categories, like determining if a image detect a human face or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6383cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import correlate2d\n",
    "\n",
    "class Convolution:\n",
    "    \n",
    "    def __init__(self, input_shape, filter_size, num_filters):\n",
    "        input_height, input_width = input_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Size of outputs and filters\n",
    "        \n",
    "        self.filter_shape = (num_filters, filter_size, filter_size) # (3,3)\n",
    "        self.output_shape = (num_filters, input_height - filter_size + 1, input_width - filter_size + 1)\n",
    "        \n",
    "        self.filters = np.random.randn(*self.filter_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        # Initialized the input value\n",
    "        output = np.zeros(self.output_shape)\n",
    "        for i in range(self.num_filters):\n",
    "            output[i] = correlate2d(self.input_data, self.filters[i], mode=\"valid\")\n",
    "        #Applying Relu Activtion function\n",
    "        output = np.maximum(output, 0)\n",
    "        return output \n",
    "    \n",
    "    def backward(self, dL_dout, lr):\n",
    "        # Create a random dL_dout array to accommodate output gradients\n",
    "        dL_dinput = np.zeros_like(self.input_data)\n",
    "        dL_dfilters = np.zeros_like(self.filters)\n",
    "\n",
    "        for i in range(self.num_filters):\n",
    "                # Calculating the gradient of loss with respect to kernels\n",
    "                dL_dfilters[i] = correlate2d(self.input_data, dL_dout[i],mode=\"valid\")\n",
    "\n",
    "                # Calculating the gradient of loss with respect to inputs\n",
    "                dL_dinput += correlate2d(dL_dout[i],self.filters[i], mode=\"full\")\n",
    "\n",
    "        # Updating the parameters with learning rate\n",
    "        self.filters -= lr * dL_dfilters\n",
    "        self.biases -= lr * dL_dout\n",
    "\n",
    "        # returning the gradient of inputs\n",
    "        return dL_dinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward(self, input_data):\n",
    "\n",
    "        self.input_data = input_data\n",
    "        self.num_channels, self.input_height, self.input_width = input_data.shape\n",
    "        self.output_height = self.input_height // self.pool_size\n",
    "        self.output_width = self.input_width // self.pool_size\n",
    "\n",
    "        # Determining the output shape\n",
    "        self.output = np.zeros((self.num_channels, self.output_height, self.output_width))\n",
    "\n",
    "        # Iterating over different channels\n",
    "        for c in range(self.num_channels):\n",
    "            # Looping through the height\n",
    "            for i in range(self.output_height):\n",
    "                # looping through the width\n",
    "                for j in range(self.output_width):\n",
    "\n",
    "                    # Starting postition\n",
    "                    start_i = i * self.pool_size\n",
    "                    start_j = j * self.pool_size\n",
    "\n",
    "                    # Ending Position\n",
    "                    end_i = start_i + self.pool_size\n",
    "                    end_j = start_j + self.pool_size\n",
    "\n",
    "                    # Creating a patch from the input data\n",
    "                    patch = input_data[c, start_i:end_i, start_j:end_j]\n",
    "\n",
    "                    #Finding the maximum value from each patch/window\n",
    "                    self.output[c, i, j] = np.max(patch)\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dL_dout, lr):\n",
    "        dL_dinput = np.zeros_like(self.input_data)\n",
    "\n",
    "        for c in range(self.num_channels):\n",
    "            for i in range(self.output_height):\n",
    "                for j in range(self.output_width):\n",
    "                    start_i = i * self.pool_size\n",
    "                    start_j = j * self.pool_size\n",
    "\n",
    "                    end_i = start_i + self.pool_size\n",
    "                    end_j = start_j + self.pool_size\n",
    "                    patch = self.input_data[c, start_i:end_i, start_j:end_j]\n",
    "\n",
    "                    mask = patch == np.max(patch)\n",
    "\n",
    "                    dL_dinput[c,start_i:end_i, start_j:end_j] = dL_dout[c, i, j] * mask\n",
    "\n",
    "        return dL_dinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e800783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fully_Connected:\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size # Size of the inputs coming\n",
    "        self.output_size = output_size # Size of the output producing\n",
    "        self.weights = np.random.randn(output_size, self.input_size)\n",
    "        self.biases = np.random.rand(output_size, 1)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        # Shift the input values to avoid numerical instability\n",
    "        shifted_z = z - np.max(z)\n",
    "        exp_values = np.exp(shifted_z)\n",
    "        sum_exp_values = np.sum(exp_values, axis=0)\n",
    "        log_sum_exp = np.log(sum_exp_values)\n",
    "\n",
    "        # Compute the softmax probabilities\n",
    "        probabilities = exp_values / sum_exp_values\n",
    "\n",
    "        return probabilities\n",
    "    \n",
    "    def softmax_derivative(self, s):\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        # Flattening the inputs from the previous layer into a vector\n",
    "        flattened_input = input_data.flatten().reshape(1, -1)\n",
    "        self.z = np.dot(self.weights, flattened_input.T) + self.biases\n",
    "\n",
    "        # Applying Softmax\n",
    "        self.output = self.softmax(self.z)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dL_dout, lr):\n",
    "        # Calculate the gradient of the loss with respect to the pre-activation (z)\n",
    "        dL_dy = np.dot(self.softmax_derivative(self.output), dL_dout)\n",
    "        # Calculate the gradient of the loss with respect to the weights (dw)\n",
    "        dL_dw = np.dot(dL_dy, self.input_data.flatten().reshape(1, -1))\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to the biases (db)\n",
    "        dL_db = dL_dy\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to the input data (dL_dinput)\n",
    "        dL_dinput = np.dot(self.weights.T, dL_dy)\n",
    "        dL_dinput = dL_dinput.reshape(self.input_data.shape)\n",
    "\n",
    "        # Update the weights and biases based on the learning rate and gradients\n",
    "        self.weights -= lr * dL_dw\n",
    "        self.biases -= lr * dL_db\n",
    "\n",
    "        # Return the gradient of the loss with respect to the input data\n",
    "        return dL_dinput\n",
    "    \n",
    "    \n",
    "def cross_entropy_loss(predictions, targets):\n",
    "\n",
    "    num_samples = 10\n",
    "\n",
    "    # Avoid numerical instability by adding a small epsilon value\n",
    "    epsilon = 1e-7\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    loss = -np.sum(targets * np.log(predictions)) / num_samples\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_loss_gradient(actual_labels, predicted_probs):\n",
    "    num_samples = actual_labels.shape[0]\n",
    "    gradient = -actual_labels / (predicted_probs + 1e-7) / num_samples\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "X_train = train_images[:5000] / 255.0\n",
    "y_train = train_labels[:5000]\n",
    "\n",
    "X_test = train_images[5000:10000] / 255.0\n",
    "y_test = train_labels[5000:10000]\n",
    "\n",
    "conv = Convolution(X_train[0].shape, 6, 1)\n",
    "pool = MaxPool(2)\n",
    "full = Fully_Connected(121, 10)\n",
    "\n",
    "def train_network(X, y, conv, pool, full, lr=0.01, epochs=200):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            # Forward pass\n",
    "            conv_out = conv.forward(X[i])\n",
    "            pool_out = pool.forward(conv_out)\n",
    "            full_out = full.forward(pool_out)\n",
    "            loss = cross_entropy_loss(full_out.flatten(), y[i])\n",
    "            total_loss += loss\n",
    "\n",
    "            # Converting to One-Hot encoding\n",
    "            one_hot_pred = np.zeros_like(full_out)\n",
    "            one_hot_pred[np.argmax(full_out)] = 1\n",
    "            one_hot_pred = one_hot_pred.flatten()\n",
    "\n",
    "            num_pred = np.argmax(one_hot_pred)\n",
    "            num_y = np.argmax(y[i])\n",
    "\n",
    "            if num_pred == num_y:\n",
    "                correct_predictions += 1\n",
    "            # Backward pass\n",
    "            gradient = cross_entropy_loss_gradient(y[i], full_out.flatten()).reshape((-1, 1))\n",
    "            full_back = full.backward(gradient, lr)\n",
    "            pool_back = pool.backward(full_back, lr)\n",
    "            conv_back = conv.backward(pool_back, lr)\n",
    "\n",
    "        # Print epoch statistics\n",
    "        average_loss = total_loss / len(X)\n",
    "        accuracy = correct_predictions / len(X_train) * 100.0\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {average_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "def predict(input_sample, conv, pool, full):\n",
    "    # Forward pass through Convolution and pooling\n",
    "    conv_out = conv.forward(input_sample)\n",
    "    pool_out = pool.forward(conv_out)\n",
    "    # Flattening\n",
    "    flattened_output = pool_out.flatten()\n",
    "    # Forward pass through fully connected layer\n",
    "    predictions = full.forward(flattened_output)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c4088",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network(X_train, y_train, conv, pool, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe51fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for data in X_test:\n",
    "    pred = predict(data, conv, pool, full)\n",
    "    one_hot_pred = np.zeros_like(pred)\n",
    "    one_hot_pred[np.argmax(pred)] = 1\n",
    "    predictions.append(one_hot_pred.flatten())\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50388c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(predictions, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
